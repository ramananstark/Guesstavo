{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jZjK2bafLbDg",
    "outputId": "a8b492aa-c434-4bbe-c8b3-acd6236eae22"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Artist</th>\n",
       "      <th>Title</th>\n",
       "      <th>Album</th>\n",
       "      <th>Year</th>\n",
       "      <th>Date</th>\n",
       "      <th>Lyric</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Taylor Swift</td>\n",
       "      <td>​cardigan</td>\n",
       "      <td>folklore</td>\n",
       "      <td>2020.0</td>\n",
       "      <td>2020-07-24</td>\n",
       "      <td>vintage tee brand new phone high heels on cobb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Taylor Swift</td>\n",
       "      <td>​exile</td>\n",
       "      <td>folklore</td>\n",
       "      <td>2020.0</td>\n",
       "      <td>2020-07-24</td>\n",
       "      <td>justin vernon i can see you standing honey wit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Taylor Swift</td>\n",
       "      <td>Lover</td>\n",
       "      <td>Lover</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>2019-08-16</td>\n",
       "      <td>we could leave the christmas lights up 'til ja...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Taylor Swift</td>\n",
       "      <td>​the 1</td>\n",
       "      <td>folklore</td>\n",
       "      <td>2020.0</td>\n",
       "      <td>2020-07-24</td>\n",
       "      <td>i'm doing good i'm on some new shit been sayin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Taylor Swift</td>\n",
       "      <td>Look What You Made Me Do</td>\n",
       "      <td>reputation</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>2017-08-25</td>\n",
       "      <td>i don't like your little games don't like your...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>474</th>\n",
       "      <td>474</td>\n",
       "      <td>Taylor Swift</td>\n",
       "      <td>Teardrops on my Guitar (Live from Clear Channe...</td>\n",
       "      <td>Live From Clear Channel Stripped 2008</td>\n",
       "      <td>2008.0</td>\n",
       "      <td>2008-06-28</td>\n",
       "      <td>drew looks at me i fake a smile so he won't se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475</th>\n",
       "      <td>475</td>\n",
       "      <td>Taylor Swift</td>\n",
       "      <td>Evermore [Forward]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020.0</td>\n",
       "      <td>2020-12-11</td>\n",
       "      <td>to put it plainly we just couldnt stop writing...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>476</th>\n",
       "      <td>476</td>\n",
       "      <td>Taylor Swift</td>\n",
       "      <td>Welcome Back Grunwald</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>turn wycd on you're on your grunwald back from...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>477</th>\n",
       "      <td>477</td>\n",
       "      <td>Taylor Swift</td>\n",
       "      <td>Tolerate it (Polskie Tłumaczenie)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020.0</td>\n",
       "      <td>2020-12-11</td>\n",
       "      <td>zwrotka  siedzę i patrzę jak czytasz z głową p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>478</th>\n",
       "      <td>478</td>\n",
       "      <td>Taylor Swift</td>\n",
       "      <td>Find you</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>trying just like they say just taking the step...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>479 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0        Artist  \\\n",
       "0             0  Taylor Swift   \n",
       "1             1  Taylor Swift   \n",
       "2             2  Taylor Swift   \n",
       "3             3  Taylor Swift   \n",
       "4             4  Taylor Swift   \n",
       "..          ...           ...   \n",
       "474         474  Taylor Swift   \n",
       "475         475  Taylor Swift   \n",
       "476         476  Taylor Swift   \n",
       "477         477  Taylor Swift   \n",
       "478         478  Taylor Swift   \n",
       "\n",
       "                                                 Title  \\\n",
       "0                                            ​cardigan   \n",
       "1                                               ​exile   \n",
       "2                                                Lover   \n",
       "3                                               ​the 1   \n",
       "4                             Look What You Made Me Do   \n",
       "..                                                 ...   \n",
       "474  Teardrops on my Guitar (Live from Clear Channe...   \n",
       "475                                 Evermore [Forward]   \n",
       "476                              Welcome Back Grunwald   \n",
       "477                  Tolerate it (Polskie Tłumaczenie)   \n",
       "478                                           Find you   \n",
       "\n",
       "                                      Album    Year        Date  \\\n",
       "0                                  folklore  2020.0  2020-07-24   \n",
       "1                                  folklore  2020.0  2020-07-24   \n",
       "2                                     Lover  2019.0  2019-08-16   \n",
       "3                                  folklore  2020.0  2020-07-24   \n",
       "4                                reputation  2017.0  2017-08-25   \n",
       "..                                      ...     ...         ...   \n",
       "474  Live From Clear Channel Stripped 2008   2008.0  2008-06-28   \n",
       "475                                     NaN  2020.0  2020-12-11   \n",
       "476                                     NaN     NaN         NaN   \n",
       "477                                     NaN  2020.0  2020-12-11   \n",
       "478                                     NaN     NaN         NaN   \n",
       "\n",
       "                                                 Lyric  \n",
       "0    vintage tee brand new phone high heels on cobb...  \n",
       "1    justin vernon i can see you standing honey wit...  \n",
       "2    we could leave the christmas lights up 'til ja...  \n",
       "3    i'm doing good i'm on some new shit been sayin...  \n",
       "4    i don't like your little games don't like your...  \n",
       "..                                                 ...  \n",
       "474  drew looks at me i fake a smile so he won't se...  \n",
       "475  to put it plainly we just couldnt stop writing...  \n",
       "476  turn wycd on you're on your grunwald back from...  \n",
       "477  zwrotka  siedzę i patrzę jak czytasz z głową p...  \n",
       "478  trying just like they say just taking the step...  \n",
       "\n",
       "[479 rows x 7 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "path=\"C:\\\\Users\\\\raman\\\\Downloads\\\\archive (1)\\\\csv\\\\TaylorSwift.csv\"\n",
    "df=pd.read_csv(path)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oe5vFy1fMBmH",
    "outputId": "da5aef3a-bb4b-408a-b036-ef0b400d6095"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "lyrics=[\"nice to meet you where you been i could show you incredible things magic madness heaven sin saw you there and i thought oh my god look at that face you look like my next mistake love's a game wanna play ayy   new money suit and tie i can read you like a magazine ain't it funny rumors fly and i know you heard about me so hey let's be friends i'm dyin' to see how this one ends grab your passport and my hand i can make the bad guys good for a weekend   so it's gonna be forever or it's gonna go down in flames you can tell me when it's over mm if the high was worth the pain got a long list of exlovers they'll tell you i'm insane 'cause you know i love the players and you love the game 'cause we're young and we're reckless we'll take this way too far it'll leave you breathless hmm or with a nasty scar got a long list of exlovers they'll tell you i'm insane but i've got a blank space baby and i'll write your name   cherry lips crystal skies i could show you incredible things stolen kisses pretty lies you're the king baby i'm your queen find out what you want be that girl for a month wait the worst is yet to come oh no  4 screaming crying perfect storms i can make all the tables turn rose garden filled with thorns keep you second guessin' like oh my god who is she i get drunk on jealousy but you'll come back each time you leave 'cause darling i'm a nightmare dressed like a daydream   so it's gonna be forever or it's gonna go down in flames you can tell me when it's over mm if the high was worth the pain got a long list of exlovers they'll tell you i'm insane 'cause you know i love the players and you love the game 'cause we're young and we're reckless we'll take this way too far it'll leave you breathless hmm or with a nasty scar got a long list of exlovers they'll tell you i'm insane but i've got a blank space baby and i'll write your name   boys only want love if it's torture don't say i didn't say i didn't warn ya boys only want love if it's torture don't say i didn't say i didn't warn ya   so it's gonna be forever or it's gonna go down in flames you can tell me when it's over mm if the high was worth the pain got a long list of exlovers they'll tell you i'm insane 'cause you know i love the players and you love the game 'cause we're young and we're reckless we'll take this way too far it'll leave you breathless hmm or with a nasty scar got a long list of exlovers they'll tell you i'm insane but i've got a blank space baby and i'll write your name\"]\n",
    "#for index,item in df.iterrows():\n",
    "  #lyric=item['Lyric']\n",
    "  #if isinstance(lyric,str):\n",
    "    #lyrics.append(lyric)\n",
    "    #for lyric in lyrics:\n",
    "      #lyric.lower().split(\"\\u2005\")\n",
    "print(len(lyrics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hH53tdriQMq9",
    "outputId": "7dfeb6df-c4cf-4e61-b737-3720f85706b2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "186\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tokenizer=Tokenizer(num_words=10000,oov_token=\"<OOv>\")\n",
    "tokenizer.fit_on_texts(lyrics)\n",
    "word_index=tokenizer.word_index\n",
    "total_words=len(word_index)+1\n",
    "print(total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "zuLgbC1ud77k"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[90, 39, 91, 2, 92, 2, 93, 5, 73, 74, 2, 75, 76, 94, 95, 96, 97, 98, 2, 99, 6, 5, 100, 40, 26, 77, 78, 101, 79, 102, 2, 78, 27, 26, 103, 104, 105, 3, 28, 106, 107, 108, 109, 110, 111, 6, 112, 5, 13, 113, 2, 27, 3, 114, 115, 116, 117, 118, 119, 6, 5, 29, 2, 120, 121, 30, 31, 122, 123, 23, 124, 8, 125, 39, 126, 127, 32, 128, 129, 130, 24, 131, 6, 26, 132, 5, 13, 80, 4, 133, 134, 135, 81, 3, 136, 31, 7, 14, 23, 41, 15, 7, 14, 42, 43, 44, 45, 2, 13, 9, 30, 46, 7, 47, 48, 25, 4, 49, 50, 51, 4, 52, 10, 3, 16, 17, 18, 19, 20, 9, 2, 8, 21, 12, 2, 29, 5, 11, 4, 53, 6, 2, 11, 4, 28, 12, 22, 54, 6, 22, 55, 56, 57, 32, 58, 59, 60, 61, 33, 2, 62, 63, 15, 34, 3, 64, 65, 10, 3, 16, 17, 18, 19, 20, 9, 2, 8, 21, 35, 66, 10, 3, 67, 68, 36, 6, 69, 70, 24, 71, 137, 138, 139, 140, 5, 73, 74, 2, 75, 76, 141, 142, 143, 144, 145, 4, 146, 36, 8, 24, 147, 148, 149, 150, 2, 72, 23, 79, 151, 81, 3, 152, 153, 4, 154, 82, 155, 39, 83, 40, 156, 157, 158, 159, 160, 161, 5, 13, 80, 162, 4, 163, 164, 165, 166, 167, 34, 168, 169, 2, 170, 171, 27, 40, 26, 77, 172, 82, 173, 5, 174, 175, 176, 177, 35, 178, 83, 179, 180, 181, 2, 33, 12, 182, 8, 3, 183, 184, 27, 3, 185, 31, 7, 14, 23, 41, 15, 7, 14, 42, 43, 44, 45, 2, 13, 9, 30, 46, 7, 47, 48, 25, 4, 49, 50, 51, 4, 52, 10, 3, 16, 17, 18, 19, 20, 9, 2, 8, 21, 12, 2, 29, 5, 11, 4, 53, 6, 2, 11, 4, 28, 12, 22, 54, 6, 22, 55, 56, 57, 32, 58, 59, 60, 61, 33, 2, 62, 63, 15, 34, 3, 64, 65, 10, 3, 16, 17, 18, 19, 20, 9, 2, 8, 21, 35, 66, 10, 3, 67, 68, 36, 6, 69, 70, 24, 71, 84, 85, 72, 11, 25, 7, 86, 87, 37, 5, 38, 37, 5, 38, 88, 89, 84, 85, 72, 11, 25, 7, 86, 87, 37, 5, 38, 37, 5, 38, 88, 89, 31, 7, 14, 23, 41, 15, 7, 14, 42, 43, 44, 45, 2, 13, 9, 30, 46, 7, 47, 48, 25, 4, 49, 50, 51, 4, 52, 10, 3, 16, 17, 18, 19, 20, 9, 2, 8, 21, 12, 2, 29, 5, 11, 4, 53, 6, 2, 11, 4, 28, 12, 22, 54, 6, 22, 55, 56, 57, 32, 58, 59, 60, 61, 33, 2, 62, 63, 15, 34, 3, 64, 65, 10, 3, 16, 17, 18, 19, 20, 9, 2, 8, 21, 35, 66, 10, 3, 67, 68, 36, 6, 69, 70, 24, 71]]\n"
     ]
    }
   ],
   "source": [
    "input_sequence=[]\n",
    "token_sequences=tokenizer.texts_to_sequences(lyrics)\n",
    "#print(token_sequences)\n",
    "\n",
    "for token in token_sequences:\n",
    "    for i in range(1,len(token)):\n",
    "        n_gram_sequences=token[:i+1]\n",
    "    #print(n_gram_sequences)\n",
    "    input_sequence.append(n_gram_sequences)\n",
    "    print(input_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tnVHe4qDfyVS"
   },
   "outputs": [],
   "source": [
    "#max_length = len(input_sequence)\n",
    "#print(max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X9Xm3RJ1xO21",
    "outputId": "ee70d7ed-cb04-47ec-f547-01a7d01b47c2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "503\n"
     ]
    }
   ],
   "source": [
    "max_sequence_length=max([len(x) for x in input_sequence])\n",
    "print(max_sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "jgqxsGlrz4ya"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 90  39  91   2  92   2  93   5  73  74   2  75  76  94  95  96  97  98\n",
      "    2  99   6   5 100  40  26  77  78 101  79 102   2  78  27  26 103 104\n",
      "  105   3  28 106 107 108 109 110 111   6 112   5  13 113   2  27   3 114\n",
      "  115 116 117 118 119   6   5  29   2 120 121  30  31 122 123  23 124   8\n",
      "  125  39 126 127  32 128 129 130  24 131   6  26 132   5  13  80   4 133\n",
      "  134 135  81   3 136  31   7  14  23  41  15   7  14  42  43  44  45   2\n",
      "   13   9  30  46   7  47  48  25   4  49  50  51   4  52  10   3  16  17\n",
      "   18  19  20   9   2   8  21  12   2  29   5  11   4  53   6   2  11   4\n",
      "   28  12  22  54   6  22  55  56  57  32  58  59  60  61  33   2  62  63\n",
      "   15  34   3  64  65  10   3  16  17  18  19  20   9   2   8  21  35  66\n",
      "   10   3  67  68  36   6  69  70  24  71 137 138 139 140   5  73  74   2\n",
      "   75  76 141 142 143 144 145   4 146  36   8  24 147 148 149 150   2  72\n",
      "   23  79 151  81   3 152 153   4 154  82 155  39  83  40 156 157 158 159\n",
      "  160 161   5  13  80 162   4 163 164 165 166 167  34 168 169   2 170 171\n",
      "   27  40  26  77 172  82 173   5 174 175 176 177  35 178  83 179 180 181\n",
      "    2  33  12 182   8   3 183 184  27   3 185  31   7  14  23  41  15   7\n",
      "   14  42  43  44  45   2  13   9  30  46   7  47  48  25   4  49  50  51\n",
      "    4  52  10   3  16  17  18  19  20   9   2   8  21  12   2  29   5  11\n",
      "    4  53   6   2  11   4  28  12  22  54   6  22  55  56  57  32  58  59\n",
      "   60  61  33   2  62  63  15  34   3  64  65  10   3  16  17  18  19  20\n",
      "    9   2   8  21  35  66  10   3  67  68  36   6  69  70  24  71  84  85\n",
      "   72  11  25   7  86  87  37   5  38  37   5  38  88  89  84  85  72  11\n",
      "   25   7  86  87  37   5  38  37   5  38  88  89  31   7  14  23  41  15\n",
      "    7  14  42  43  44  45   2  13   9  30  46   7  47  48  25   4  49  50\n",
      "   51   4  52  10   3  16  17  18  19  20   9   2   8  21  12   2  29   5\n",
      "   11   4  53   6   2  11   4  28  12  22  54   6  22  55  56  57  32  58\n",
      "   59  60  61  33   2  62  63  15  34   3  64  65  10   3  16  17  18  19\n",
      "   20   9   2   8  21  35  66  10   3  67  68  36   6  69  70  24  71]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "padded_input_sequences=pad_sequences(input_sequence,maxlen=max_sequence_length,padding='pre')\n",
    "padded_input_sequences=np.array(padded_input_sequences)\n",
    "print(padded_input_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "1LTsy3Y0iAuH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow\n",
    "xs=padded_input_sequences[:,:-1]\n",
    "labels=padded_input_sequences[:,-1]\n",
    "ys=tensorflow.keras.utils.to_categorical(labels,num_classes=total_words)\n",
    "print(ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "iEJFtbzwvG5l",
    "outputId": "92227122-2947-4be1-9b38-6bc88b95007e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1/1 [==============================] - 5s 5s/step - loss: 5.2337 - accuracy: 0.0000e+00\n",
      "Epoch 2/20\n",
      "1/1 [==============================] - 1s 558ms/step - loss: 5.2097 - accuracy: 0.0000e+00\n",
      "Epoch 3/20\n",
      "1/1 [==============================] - 1s 545ms/step - loss: 5.1857 - accuracy: 1.0000\n",
      "Epoch 4/20\n",
      "1/1 [==============================] - 1s 534ms/step - loss: 5.1617 - accuracy: 1.0000\n",
      "Epoch 5/20\n",
      "1/1 [==============================] - 1s 547ms/step - loss: 5.1376 - accuracy: 1.0000\n",
      "Epoch 6/20\n",
      "1/1 [==============================] - 1s 541ms/step - loss: 5.1135 - accuracy: 1.0000\n",
      "Epoch 7/20\n",
      "1/1 [==============================] - 1s 563ms/step - loss: 5.0894 - accuracy: 1.0000\n",
      "Epoch 8/20\n",
      "1/1 [==============================] - 1s 560ms/step - loss: 5.0652 - accuracy: 1.0000\n",
      "Epoch 9/20\n",
      "1/1 [==============================] - 0s 495ms/step - loss: 5.0410 - accuracy: 1.0000\n",
      "Epoch 10/20\n",
      "1/1 [==============================] - 1s 585ms/step - loss: 5.0166 - accuracy: 1.0000\n",
      "Epoch 11/20\n",
      "1/1 [==============================] - 1s 573ms/step - loss: 4.9922 - accuracy: 1.0000\n",
      "Epoch 12/20\n",
      "1/1 [==============================] - 1s 541ms/step - loss: 4.9676 - accuracy: 1.0000\n",
      "Epoch 13/20\n",
      "1/1 [==============================] - 1s 526ms/step - loss: 4.9428 - accuracy: 1.0000\n",
      "Epoch 14/20\n",
      "1/1 [==============================] - 1s 563ms/step - loss: 4.9179 - accuracy: 1.0000\n",
      "Epoch 15/20\n",
      "1/1 [==============================] - 1s 530ms/step - loss: 4.8928 - accuracy: 1.0000\n",
      "Epoch 16/20\n",
      "1/1 [==============================] - 1s 504ms/step - loss: 4.8676 - accuracy: 1.0000\n",
      "Epoch 17/20\n",
      "1/1 [==============================] - 0s 469ms/step - loss: 4.8421 - accuracy: 1.0000\n",
      "Epoch 18/20\n",
      "1/1 [==============================] - 1s 520ms/step - loss: 4.8163 - accuracy: 1.0000\n",
      "Epoch 19/20\n",
      "1/1 [==============================] - 1s 521ms/step - loss: 4.7904 - accuracy: 1.0000\n",
      "Epoch 20/20\n",
      "1/1 [==============================] - 1s 549ms/step - loss: 4.7641 - accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Bidirectional,LSTM\n",
    "model = tensorflow.keras.Sequential([\n",
    "    keras.layers.Embedding(total_words,240,input_length=max_sequence_length-1),\n",
    "    keras.layers.Bidirectional(LSTM(150)),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(total_words,activation='softmax'),\n",
    "])\n",
    "\n",
    "model.compile(optimizer='sgd',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "history=model.fit(xs,ys,epochs=20,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "V9xnGn2BF0cf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "so callout my name mm\n",
      "so callout my name mm so\n",
      "so callout my name mm so or\n",
      "so callout my name mm so or heard\n",
      "so callout my name mm so or heard meet\n",
      "so callout my name mm so or heard meet rumors\n",
      "so callout my name mm so or heard meet rumors turn\n",
      "so callout my name mm so or heard meet rumors turn tie\n",
      "so callout my name mm so or heard meet rumors turn tie write\n",
      "so callout my name mm so or heard meet rumors turn tie write nightmare\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "seed_text = input(\"Enter a sequence of text: \")\n",
    "number_of_words = int(input(\"Enter the number of words required: \"))\n",
    "\n",
    "for i in range(number_of_words):\n",
    "    token = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "    padded_token = pad_sequences([token], padding='pre', maxlen=max_sequence_length - 1)\n",
    "\n",
    "    predicted = model.predict(padded_token, verbose=0)\n",
    "    \n",
    "    # Sample a word index from the predicted distribution\n",
    "    predicted_index = np.random.choice(len(predicted[0]), p=predicted[0])\n",
    "\n",
    "    output_word = \"\"\n",
    "\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == predicted_index:\n",
    "            output_word = word\n",
    "            break\n",
    "\n",
    "    seed_text += \" \" + output_word\n",
    "    print(seed_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
